# -*- coding: utf-8 -*-
"""Forecast_one_to_one.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WnZxz6etltpnWntLjNeCjBMgOyarpbt2
"""

from google.colab import drive
drive.mount('/content/gdrive')

base_directory = '/content/gdrive/My Drive/ML_MinChi/Assignment 1/'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from dateutil.parser import parse
from datetime import datetime
import os
import glob
import seaborn as sns
import matplotlib as mpl
plt.ion()
sns.set(rc={'figure.figsize':(20, 4)})
mpl.rcParams['agg.path.chunksize'] = 10000

def getDirectory(directory):
    directory = directory.replace(" / ", "_")
    os.chdir(base_directory)
    if not os.path.exists(directory):
            os.makedirs(directory)
    os.chdir('./' + directory)
    
def groupDataframe(data, col='', directory=''):
    getDirectory(directory)
    for index, df in data.groupby(col):
        df['Date'] =  [parse(week) for week in df.Week]
        file_key = 'df_' + str(index).replace(" / ", "_")
        df.name = file_key
        df.to_hdf(file_key + '.h5', key = file_key, mode='w')
#         df.to_csv(file_key + '.csv')

# def loadDataframe(df_name='', directory=''):
#     getDirectory(directory)
#     if len(df_name.split('.')) == 1:
#         hdf_filename = 'df_' + df_name + '.h5'
#     else:
#         hdf_filename = df_name
# #     print(hdf_filename)
#     df = pd.read_hdf(hdf_filename)
#     return df

def loadDataframe(df_name='', directory=''):
    getDirectory(directory)
    df = pd.read_csv(df_name)
    return df

def plot_comparison(df, col1, col2):
    plt.figure(figsize=(12,5))
    plt.xlabel('Days')

    ax1 = df[col1].plot(marker = 'o', markersize = 4, color = 'blue', grid=True, label=col1)
    ax2 = df[col2].plot(marker = 'x',  markersize=12, color = 'blue', grid=True, label=col2)

    h1, l1 = ax1.get_legend_handles_labels()
    h2, l2 = ax2.get_legend_handles_labels()

    plt.legend(h1+h2, l1+l2, loc=2)
    plt.savefig(col1 + '_' + col2 + '.png')
    plt.show()
        
def saveDataframe(df, name_dir='New Folder', name_file='Untitled'):
    getDirectory(name_dir)
    print('name_file:', name_file)
    df.to_csv(name_file + '.csv')

def get_file_list(path, ext='csv'):
    result = glob.glob('*.{}'.format(ext))
    return result

def get_normalization_factor(array):
    try:
        c = len(array)
        s = sum(array)
        normalization_factor = 1/(1+(s/c))
    except ZeroDivisionError:
        normalization_factor = 0
    return normalization_factor
    
def mean_scale_normalize(array):
    normalization_factor = get_normalization_factor(array)
    return np.array(array) * normalization_factor

def inverse_mean_scale_normalize(array, normalization_factor):
    return np.array(array) / normalization_factor

def inverse_mean_scale_normalize_cluster(array, normalization_factor):
    return np.array(array) / np.array(normalization_factor)

def moving_window_batch_generator(X_train, Y_train, batch_size=256, input_sequence_length=2, output_sequence_length=2):
    num_X_features = X_train.shape[1]
    num_Y_features = Y_train.shape[1]
    train_size = Y_train.shape[0]
    print('train size', train_size)
    print('input_sequence_length', input_sequence_length)
    
    while True:
        X_shape = (batch_size, input_sequence_length, num_X_features)
        X_train_batch = np.zeros(shape=X_shape, dtype=np.float16)
        
        Y_shape = (batch_size, output_sequence_length, num_Y_features)
        Y_tain_batch = np.zeros(shape=Y_shape, dtype=np.float16)

        for i in range(batch_size):
            id = np.random.randint(train_size - input_sequence_length)
            X_train_batch[i] = X_train[id:id+input_sequence_length]
            Y_tain_batch[i] = Y_train[id:id+output_sequence_length]
        
        yield (X_train_batch, Y_tain_batch)

from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau

def get_callbacks(file_name = ''):
    if file_name != '':
        path_checkpoint ='checkpoint_keras_'+ file_name
        log_dir='logs_' +  file_name
    
    callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint, 
                                          monitor='val_loss', 
                                          verbose=1, 
                                          save_weights_only=False, 
                                          save_best_only=True, 
                                          mode='auto', 
                                          period=1)
    callback_early_stopping = EarlyStopping(monitor='val_loss', 
                                            patience=5, 
                                            verbose=1)
    callback_tensorboard = TensorBoard(log_dir=log_dir, 
                                       histogram_freq=0, 
                                       write_graph=False)
    callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss', 
                                           factor=0.1, 
                                           min_lr=1e-4, 
                                           patience=3, 
                                           verbose=1)

    callbacks = [callback_checkpoint, callback_tensorboard, callback_reduce_lr]

    return callbacks

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, LSTM, Dropout, GaussianNoise, BatchNormalization, TimeDistributed, Flatten
from tensorflow.python.keras.optimizers import Adam
from tensorflow.python.keras.regularizers import l2

def get_model(hyperparameters, predictors=[]):

    # Initialising the RNN
    model = Sequential()
    regularizer = l2(0.01)
    optimizer = Adam(lr=hyperparameters['learning_rate'])

    model.add(
        LSTM(units = 30,  
                  input_shape=(hyperparameters['input_sequence_length'], len(predictors)),
                  return_sequences = True,
                  kernel_regularizer=regularizer))
    model.add(GaussianNoise(1e-4))
#     model.add(BatchNormalization())
    
    model.add(
        LSTM(units = 20, 
             return_sequences = True,
             kernel_regularizer=regularizer))
#     model.add(GaussianNoise(1e-4))
#     model.add(BatchNormalization())
    
    model.add(
        LSTM(units = 10,
             kernel_regularizer=regularizer, return_sequences = False))
    model.add(GaussianNoise(1e-4))
#     model.add(BatchNormalization())
    
    model.add(Dense(hyperparameters['output_sequence_length'], activation='relu'))
    
    model.compile(optimizer = optimizer, loss = 'mean_squared_error')

    print(model.summary())
    
    return model

def plot_comparison(df, acct, msku):
    plt.figure(figsize=(12,5))
    plt.xlabel('Dates')

    ax1 = df.Y_true.plot(color='blue', grid=True, label='True')
    ax2 = df.Y_pred.plot(color='red', grid=True, label='Predicted')

    h1, l1 = ax1.get_legend_handles_labels()
    h2, l2 = ax2.get_legend_handles_labels()

    plt.legend(h1+h2, l1+l2, loc=2)
    plt.savefig(msku.split('.')[0] + '.png')
    plt.show()

def get_keras_shape(array, time_steps = 1, num_features=1):
    batch_size = array.shape[0]
    return array.reshape((batch_size, time_steps, num_features))
  

def moving_window_data_formatter_train(X, Y, train_size, hyperparameters):
    print('train_size:',train_size)
    X = X.values
    Y = Y.values
    X_train, Y_train = [], []
    for i in range(train_size):
        X_start_index = i
        X_end_index = i + hyperparameters['input_sequence_length']
        Y_start_index = i + hyperparameters['input_sequence_length']
        Y_end_index = i + hyperparameters['input_sequence_length'] + hyperparameters['output_sequence_length']  
        if Y_end_index <= train_size:
            X_train.append(X[X_start_index:X_end_index])
            Y_train.append(Y[Y_start_index:Y_end_index])
    X_train, Y_train = np.array(X_train), np.array(Y_train)
    return X_train, Y_train

from sklearn.preprocessing import MinMaxScaler

def normalize_dataframe(df, cols = []):
    for col in cols:
        col_scaled = MinMaxScaler().fit_transform(df[[col]].values.astype(float))
        df['normalized_' + col] = np.array(col_scaled).reshape((-1,1))
    return df


def reverse_normalize(col_original, col_normalized):
    print('col_original', col_original.shape)
    print('col_normalized', col_normalized.shape)
    scaler = MinMaxScaler().fit(col_original.reshape(-1, 1))
    return scaler.inverse_transform(col_normalized.reshape(-1, 1))
  
  
  
def get_predicted_values(X, col_original, model, hyperparameters):
    results = []
    Y_pred = model.predict(X)
    print('Y.shape', Y_pred.shape)
    results = reverse_normalize(np.array(col_original), np.array(Y_pred))
#     results = np.concatenate((col_original.reshape(-1, 1)[: hyperparameters['input_sequence_length']] , results), axis=0) 
    print("results:", results.shape)
    return results

from pandas.tseries.holiday import USFederalHolidayCalendar
from pandas import Timestamp

def addExogenousVariables(df):
    holidays = USFederalHolidayCalendar().holidays(start=df.index.min(), end=df.index.max())
    df['holiday'] = df.index.isin(holidays).astype(int)
    df['week_number'] = df.index.week
    df['day_of_week'] = df.index.dayofweek
    return df


def fit_selected_dataframe():
    #Hyperparameters
    hyperparameters = {
        'cell_dimension': 10,
        'batch_size': 128,
        'input_sequence_length': 5,
        'output_sequence_length': 1,
        'learning_rate': 1e-2,
        'epochs': 20,
        'gaussian_noise': 1e-2,
        'L2_regularization': 1e-4
    }

    getDirectory('train_data/processed')
    file_list = get_file_list('', ext='csv')
    file_list = [file for file in file_list if file not in patient_exists]
    print('file_list', file_list)
    results = []
    for patient in file_list:
        df = pd.DataFrame()
        
        df = loadDataframe(patient, directory='train_data/processed/')
        df_one_to_one = pd.DataFrame(index = df.index)
        df = df.fillna(value=-1)
        df = normalize_dataframe(df, cols = cols)
        predictors = ['normalized_' + col for col in cols]
        
    
        print('--------------Selected-----------------', patient)

        for index, predictor in enumerate(predictors):
            print('--------------Selected_predictor-----------------', predictor)
            original_col = predictor.replace('normalized_', '')
            X = df[predictor]
            Y = df[predictor]
            
            test_size = hold_out
            train_size = len(df.index) - test_size
#             prediction_size = forecast_days - len(df.index)

            X_train, Y_train = moving_window_data_formatter_train(X, Y, train_size, hyperparameters)

            print('Shape X before reshape', X_train.shape)

            X_train = get_keras_shape(X_train, 
                                      time_steps=hyperparameters['input_sequence_length'], 
                                      num_features = 1)

#             X_test = np.array(X.iloc[-(hyperparameters['input_sequence_length'] + test_size): -test_size])
#             X_test = np.reshape(X_test, (-1, 1))
#             X_test = np.array([X_test])

            print('Shape X', X_train.shape)
            print('Shape Y', Y_train.shape)
#             print('Shape X test', X_test.shape)

            model = get_model(hyperparameters, predictors = [predictor])
            getDirectory('models/one_to_one/' +  patient.split('.')[0] + '/' + 'keras_'+ predictor)


            if X_train.shape[0] > 1:
                model.fit(X_train, Y_train,
                          validation_split=0.25,
                          epochs = hyperparameters['epochs'],
                          batch_size = hyperparameters['batch_size'],
                          callbacks = get_callbacks(file_name = predictor))


            try:
                path_checkpoint = 'checkpoint_keras_' + predictor
                model.load_weights(path_checkpoint)
            except Exception as error:
                print("Error trying to load checkpoint.")
                print(error)
            
            results = get_predicted_values(X_train, df[original_col], model, hyperparameters)
            rolloing_mean6 = 'rolling_mean6_' + original_col
            top_values = np.array(df[rolloing_mean6])[:hyperparameters['input_sequence_length']].reshape(-1, 1)
#             print('top_values:', top_values)
            results = np.concatenate((top_values, results), axis=0) 
            print('results.shape', results.shape)
            
            df_one_to_one[original_col+'_one_to_one'] = results
        getDirectory('train_data/processed_one_to_one');
        saveDataframe(df_one_to_one, 
                      name_dir = 'train_data/processed_one_to_one', 
                      name_file = patient.split('.')[0])

getDirectory('train_data/processed')
processed_data_files = glob.glob("*")
cols  = ['X' + str(i) + '_missing_data' for i in range(1, 14)]
hold_out = 0
getDirectory('train_data/processed_one_to_one')
patient_exists = glob.glob("*")
patient_exists = [s for s in patient_exists]
print("patient_exists exist:", patient_exists)

fit_selected_dataframe()

# import json

# getDirectory('Account_Msku_Grp')
# accounts = glob.glob("*")
# print(accounts)
# # accounts = ['df_300018']
# weeks = 30
# end_date = '2019-01-31'
# df_selected = {}

# for acct in accounts:
#     print(acct)
#     time1 = datetime.now()
#     df_dict = fit_selected_dataframe(acct, type='q_95')
#     getDirectory('Account_Msku_Grp/' + acct + '/Forecast_approach1/others_D' + '/' + 'q_95')
#     with open('results.json', 'w') as fp:
#         json.dump(df_dict, fp, sort_keys=True, indent=4)
#     avg_error = sum(results)/(len(results) - 1)
#     time2 = datetime.now()  
#     elapsedTime = time2 - time1
#     print('elapsed time:', divmod(elapsedTime.total_seconds(), 60))
#     print('avg_error:', avg_error)



