# -*- coding: utf-8 -*-
"""train_initial_baseline_solutions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NxsUyOnt-ExS7BUJMbCtZFpo_m9_8eBp
"""

'''
train using baselines like mean, median, rolling window
'''

base_directory = '/content/gdrive/My Drive/ML_MinChi/Assignment 1/'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from dateutil.parser import parse
from datetime import datetime
import os
import glob
import seaborn as sns
import matplotlib as mpl
plt.ion()
sns.set(rc={'figure.figsize':(20, 4)})
mpl.rcParams['agg.path.chunksize'] = 10000

base_directory = os.getcwd().replace('\\', '/') + '/'

def getDirectory(directory):
    directory = directory.replace(" / ", "_")
    os.chdir(base_directory)
    if not os.path.exists(directory):
            os.makedirs(directory)
    os.chdir('./' + directory)
    
def groupDataframe(data, col='', directory=''):
    getDirectory(directory)
    for index, df in data.groupby(col):
        df['Date'] =  [parse(week) for week in df.Week]
        file_key = 'df_' + str(index).replace(" / ", "_")
        df.name = file_key
        df.to_hdf(file_key + '.h5', key = file_key, mode='w')

def loadDataframe(df_name='', directory=''):
    getDirectory(directory)
    df = pd.read_csv(df_name)
    return df

def plot_comparison(df, col1, col2):
    plt.figure(figsize=(12,5))
    plt.xlabel('Days')

    ax1 = df[col1].plot(marker = 'o', markersize = 4, color = 'blue', grid=True, label=col1)
    ax2 = df[col2].plot(marker = 'x',  markersize=12, color = 'blue', grid=True, label=col2)

    h1, l1 = ax1.get_legend_handles_labels()
    h2, l2 = ax2.get_legend_handles_labels()

    plt.legend(h1+h2, l1+l2, loc=2)
    plt.savefig(col1 + '_' + col2 + '.png')
    plt.show()
        
def saveDataframe(df, name_dir='New Folder', name_file='Untitled'):
    getDirectory(name_dir)
    df.to_csv(name_file + '.csv')

def get_file_list(path, ext='csv'):
    result = glob.glob('*.{}'.format(ext))
    return result

from datetime import datetime, timedelta

def trim_front_end(df):
    if len(np.flatnonzero(df.AdjShip)) > 0:
        first_nonzero_row = np.flatnonzero(df.AdjShip)[0]
        last_nonzero_row = np.flatnonzero(df.AdjShip)[-1]
    else:
        first_nonzero_row = len(df.index) - 1
        last_nonzero_row = len(df.index) - 1
    df_front_trim_end = df.iloc[first_nonzero_row:]
#     print(df_front_trim_end.tail(1))
    return df_front_trim_end

def is_selected(df, end_date):
    w = str(weeks) + 'W'
    df_last = df.iloc[-1:]
    print(df_last)
    if df_last.index >= datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=7):
        return True
    else:
        return False

getDirectory('train_data/processed')
processed_data_files = glob.glob("*")
cols  = ['X' + str(i) + '_missing_data' for i in range(1, 14)]

for index in range(1695, len(processed_data_files)):
  print(index+1)
  getDirectory('train_data/processed')
  df_processed_data = pd.read_csv(processed_data_files[index])

  for col in cols:

    # Filling using mean or median
    df_processed_data['fill_mean_' + col] = df_processed_data[col].fillna(df_processed_data[col].mean())
    df_processed_data['fill_median_' + col] = df_processed_data[col].fillna(df_processed_data[col].median())

    # imputing using the rolling average
    df_processed_data['rolling_mean6_' + col] = df_processed_data[col].fillna(df_processed_data[col].rolling(6,min_periods=1,).mean())

    # imputing using the rolling median
    df_processed_data['rolling_median6_' + col] = df_processed_data[col].fillna(df_processed_data[col].rolling(6,min_periods=1,).median())

    # Imputing using interpolation with different methods
    df_processed_data['interpolate_linear_' + col] = df_processed_data[col].interpolate(method='linear')
#     df_processed_data['interpolate_time_' + col] = df_processed_data.col.interpolate(method='time')
    df_processed_data['interpolate_quadratic_' + col] = df_processed_data[col].interpolate(method='quadratic')
    df_processed_data['interpolate_quadratic_' + col] = df_processed_data[col].interpolate(method='cubic')
    df_processed_data['interpolate_slinear_' + col] = df_processed_data[col].interpolate(method='slinear')
    df_processed_data['interpolate_akima_' + col] = df_processed_data[col].interpolate(method='akima')
    df_processed_data['interpolate_poly5_' + col] = df_processed_data[col].interpolate(method='polynomial', order=5)
    df_processed_data['interpolate_poly7_' + col] = df_processed_data[col].interpolate(method='polynomial', order=7)
    df_processed_data['interpolate_spline3_' + col] = df_processed_data[col].interpolate(method='spline', order=3)
    df_processed_data['interpolate_spline4_' + col] = df_processed_data[col].interpolate(method='spline', order=4)
    df_processed_data['interpolate_spline5_' + col] = df_processed_data[col].interpolate(method='spline', order=5)

  saveDataframe(df_processed_data, name_dir='train_data/processed', name_file = str(index+1))



getDirectory('train_data/processed')
processed_data_files = glob.glob("*")
cols  = ['X' + str(i) + '_missing_data' for i in range(1, 14)]

for index in range(len(processed_data_files)):
  print(index+1)
  getDirectory('train_data/processed')
  df_processed_data = pd.read_csv(processed_data_files[index])
